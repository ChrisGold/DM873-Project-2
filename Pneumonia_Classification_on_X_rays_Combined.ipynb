{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Pneumonia_Classification_on_X-rays_Combined.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4oM-lVBu3K7",
        "outputId": "43fb6ce6-5bcb-4404-98f5-4637c2fd2757"
      },
      "source": [
        "# Only for colab\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cffsxgv-qmww"
      },
      "source": [
        "import os, shutil\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import preprocessing\n",
        "import os, shutil, glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "945vKEfGqmw4"
      },
      "source": [
        "## Load data\n",
        "### Make sure this file and original dataset in the same directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8RzOG1Mqmw4"
      },
      "source": [
        "# Directory in which we store our dataset\n",
        "\n",
        "# Old dataset path\n",
        "original_dataset_dir = 'drive/My Drive/lung/filtered'\n",
        "\n",
        "# New dataset path\n",
        "base_dir = 'drive/My Drive/lung/new_set'\n",
        "# Create a new directory\n",
        "#os.mkdir(base_dir)   # comment this line it if you've run it"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iA_JZwRqmw5"
      },
      "source": [
        "# Directory in which we store training, validation\n",
        "# And Create directories of training and validation for normal and pneumonia respectively \n",
        "\n",
        "train_dir = os.path.join(base_dir,'train')\n",
        "#os.mkdir(train_dir)               # comment this line it if you've run it\n",
        "\n",
        "validation_dir = os.path.join(base_dir,'validation')\n",
        "#os.mkdir(validation_dir)          # comment this line it if you've run it\n",
        "\n",
        "\n",
        "train_normal_dir = os.path.join(train_dir, 'NORMAL')\n",
        "#os.mkdir(train_normal_dir)        # comment this line it if you've run it\n",
        "\n",
        "train_pneumonia_dir = os.path.join(train_dir, 'PNEUMONIA')\n",
        "#os.mkdir(train_pneumonia_dir)     # comment this line it if you've run it\n",
        "\n",
        "validation_normal_dir = os.path.join(validation_dir, 'NORMAL')\n",
        "#os.mkdir(validation_normal_dir)   # ccomment this line it if you've run it\n",
        "\n",
        "validation_pneumonia_dir = os.path.join(validation_dir, 'PNEUMONIA')\n",
        "#os.mkdir(validation_pneumonia_dir)# comment this line it if you've run it"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmzsUwNZqmw5"
      },
      "source": [
        "# All jpeg data from old directory\n",
        "imageList_normal = glob.glob(os.path.join( (original_dataset_dir + '/NORMAL/'), '*.jpeg'))\n",
        "imageList_pneumonia = glob.glob(os.path.join( (original_dataset_dir + '/PNEUMONIA/'), '*.jpeg'))\n",
        "\n",
        "# All jpeg data from new directories\n",
        "imageList_train_normal = glob.glob(os.path.join( (base_dir + '/train/NORMAL/'), '*.jpeg'))\n",
        "imageList_val_normal = glob.glob(os.path.join( (base_dir + '/validation/NORMAL/'), '*.jpeg'))\n",
        "\n",
        "imageList_train_pneumonia = glob.glob(os.path.join(  (base_dir + '/train/PNEUMONIA/'), '*.jpeg'))\n",
        "imageList_val_pneumonia = glob.glob(os.path.join(  (base_dir + '/validation/PNEUMONIA/'), '*.jpeg'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ixPT28qmw5"
      },
      "source": [
        "def is_file(fname_from,imageList):\n",
        "    check=False\n",
        "    for f in imageList: \n",
        "        fname_to = os.path.basename(f)\n",
        "        if(fname_to == fname_from):\n",
        "            check=True\n",
        "    return check"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRuQXIs9qmw6"
      },
      "source": [
        "\n",
        "normal_cases_dir =   (original_dataset_dir + '/NORMAL/')    # Old normal path\n",
        "pneumonia_cases_dir = (original_dataset_dir + '/PNEUMONIA/')# Old pneumonia path\n",
        "\n",
        "\n",
        "LENTH_DATASET=len(imageList_normal)                # total normal images (as the same as pneumonia images)\n",
        "\n",
        "split_train_percent=int(LENTH_DATASET*0.8)   #0.8 for train data\n",
        "split_val_percent=int(LENTH_DATASET-split_train_percent)  #0.2 for train data\n",
        "\n",
        "count=0\n",
        "\n",
        "# Split normal data into 80% train and 20% validation\n",
        "for f in imageList_normal:\n",
        "    count+=1    \n",
        "    fname = os.path.basename(f)\n",
        "    if(count<=split_train_percent and is_file(fname,imageList_train_normal)==False):\n",
        "        src = os.path.join(normal_cases_dir, fname)\n",
        "        dst = os.path.join(train_normal_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    elif(count>split_train_percent and is_file(fname,imageList_val_normal)==False):\n",
        "        src = os.path.join(normal_cases_dir, fname)\n",
        "        dst = os.path.join(validation_normal_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    \n",
        "count=0\n",
        "\n",
        "# Split pneumonia data into 80% train and 20% validation\n",
        "for f in imageList_pneumonia:\n",
        "    count+=1    \n",
        "    fname = os.path.basename(f)\n",
        "    if(count<=split_train_percent and is_file(fname,imageList_train_pneumonia)==False):\n",
        "        src = os.path. join(pneumonia_cases_dir, fname)\n",
        "        dst = os.path.join(train_pneumonia_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    elif(count>split_train_percent and is_file(fname,imageList_val_pneumonia)==False):\n",
        "        src = os.path.join(pneumonia_cases_dir, fname)\n",
        "        dst = os.path.join(validation_pneumonia_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDyDg-_bqmw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b7d6b5-44b7-4d79-be2d-aea8b1cc13a1"
      },
      "source": [
        "print('total training normal images:', len(os.listdir(train_normal_dir)))\n",
        "print('total training pneumonia images:', len(os.listdir(train_pneumonia_dir)))\n",
        "\n",
        "print('total training normal images:', len(os.listdir(validation_normal_dir)))\n",
        "print('total training pneumonia images:', len(os.listdir(validation_pneumonia_dir)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training normal images: 1072\n",
            "total training pneumonia images: 1082\n",
            "total training normal images: 273\n",
            "total training pneumonia images: 269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "facOl3UPqmw8"
      },
      "source": [
        "# Structure of directory\n",
        "\n",
        "BRANCH = '├─'\n",
        "LAST_BRANCH = '└─'\n",
        "TAB = '│  '\n",
        "EMPTY_TAB = '   '\n",
        "\n",
        "\n",
        "def get_dir_list(path, placeholder=''):\n",
        "    folder_list = [folder for folder in os.listdir(path) if os.path.isdir(os.path.join(path, folder))]\n",
        "    file_list = [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
        "    result = ''\n",
        "    for folder in folder_list[:-1]:\n",
        "        result += placeholder + BRANCH + folder + '\\n'\n",
        "        result += get_dir_list(os.path.join(path, folder), placeholder + TAB)\n",
        "    if folder_list:\n",
        "        result += placeholder + (BRANCH if file_list else LAST_BRANCH) + folder_list[-1] + '\\n'\n",
        "        result += get_dir_list(os.path.join(path, folder_list[-1]), placeholder + (TAB if file_list else EMPTY_TAB))\n",
        "    return result\n",
        "if __name__ == '__main__':\n",
        "    print(get_dir_list('./new_set'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiN8ungSqmw8"
      },
      "source": [
        "## Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMMxaoV5qmw8"
      },
      "source": [
        "from glob import glob\n",
        "import cv2\n",
        "print(\"No Pneumonia\")\n",
        "multipleImages = glob('./new_set/train/NORMAL/**')\n",
        "i_ = 0\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "for l in multipleImages[:10]:\n",
        "    im = cv2.imread(l)\n",
        "    im = cv2.resize(im, (128, 128)) \n",
        "    plt.subplot(5, 5, i_+1) \n",
        "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
        "    i_ += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwOMcen9qmw9"
      },
      "source": [
        "print(\"Pneumonia\")\n",
        "multipleImages = glob('./new_set/train/PNEUMONIA/**')\n",
        "i_ = 0\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "for l in multipleImages[:10]:\n",
        "    im = cv2.imread(l)\n",
        "    im = cv2.resize(im, (128, 128)) \n",
        "    plt.subplot(5, 5, i_+1) \n",
        "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
        "    i_ += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlDC2Kzuqmw-"
      },
      "source": [
        "## DataGenerator and Image augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14l54pH5qmw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3a23f7-db52-4591-d6e8-0339d48293ea"
      },
      "source": [
        "image_width = 224\n",
        "image_height = 224\n",
        "batch=128\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,fill_mode = 'nearest',\n",
        "    \n",
        "    #channel_shift_range=13,data_format='channels_last',\n",
        ")\n",
        "print((split_train_percent*2)//batch)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(image_width, image_height),\n",
        "    batch_size=batch,\n",
        "    color_mode='grayscale',     # Channel 3 to channel 1\n",
        "    class_mode='binary',\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(image_width, image_height),\n",
        "    batch_size=batch,          # Channel 3 to channel 1\n",
        "     color_mode='grayscale',\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "trainAll_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(image_width, image_height),\n",
        "    batch_size=batch,\n",
        "    color_mode='grayscale',     # Channel 3 to channel 1\n",
        "    class_mode='binary',\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Found 2154 images belonging to 2 classes.\n",
            "Found 542 images belonging to 2 classes.\n",
            "Found 2696 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkYYuYgQqmw_"
      },
      "source": [
        "# Task 3 - Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-YVkZvbqmw_"
      },
      "source": [
        "# Display how many test data respect to normal and pneumonia\n",
        "test_dir = './encoded'\n",
        "\n",
        "test_normal_dir = os.path.join(test_dir, 'NORMAL')\n",
        "test_pneumonia_dir = os.path.join(test_dir, 'PNEUMONIA')\n",
        "\n",
        "print('total test normal images:', len(os.listdir(test_normal_dir)))\n",
        "print('total test pneumonia images:', len(os.listdir(test_pneumonia_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjSj343zqmxA"
      },
      "source": [
        "# Read the test text file and store into list\n",
        "# And split into x_test and y_test for labels\n",
        "test_dir = './encoded'\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['NORMAL', 'PNEUMONIA']:    \n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            tex = []\n",
        "            for line in f:\n",
        "                row = line.split()\n",
        "                for r in row:\n",
        "                    r1 = float(r)  # Convert string to float\n",
        "                    tex.append(r1) # (50176)\n",
        "                                               \n",
        "            texts.append(tex)      # (51*50176)\n",
        "            if label_type == 'NORMAL':\n",
        "                labels.append(0)   # Labled Normal to 0     \n",
        "            else:\n",
        "                labels.append(1)   # Labled PNEUMONIA to 1         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeqYQakpqmxA"
      },
      "source": [
        "# Convert list to np array in order to reshape\n",
        "x_test=np.array(texts)\n",
        "y_test=np.array(labels)\n",
        "print(x_test.shape)\n",
        "\n",
        "# Reshape as the same format as train\n",
        "x_test=x_test.reshape(51,224,224,1) \n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "y7qOZaUeqmxB"
      },
      "source": [
        "model.load_weights('pre_trained_model.h5')\n",
        "# Choose one of them\n",
        "test_iterator = test_datagen.flow(x_test, y_test, batch_size=batch)\n",
        "test_loss, test_acc =model.evaluate(x_test, y_test, steps=46)\n",
        "\n",
        "print('test acc:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cabAP9zPKWdG"
      },
      "source": [
        "# Custom Layers\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g_gXj7-MEGS"
      },
      "source": [
        "from keras.utils import conv_utils\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import activations\r\n",
        "from keras import backend as K\r\n",
        "from typing import Any, Union\r\n",
        "import keras.backend as K\r\n",
        "from keras import activations\r\n",
        "from tensorflow.python.distribute.sharded_variable import ShardedVariable\r\n",
        "from tensorflow.python.ops.variables import PartitionedVariable"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDLyhTPEKa7D"
      },
      "source": [
        "class MaxPooling(Layer):\r\n",
        "    def __init__(self, pool_size=(2, 2), strides=None, padding='VALID', **kwargs):\r\n",
        "        super(MaxPooling, self).__init__(**kwargs)\r\n",
        "        self.pool_size = pool_size\r\n",
        "        self.strides = strides\r\n",
        "        self.padding = padding\r\n",
        "\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        super(MaxPooling, self).build(input_shape)\r\n",
        "\r\n",
        "    def call(self, x, **kwargs):\r\n",
        "        y = tf.nn.max_pool(x, self.pool_size, self.strides, self.padding)\r\n",
        "        return y\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        conv_len = conv_utils.conv_output_length(input_shape[1], self.pool_size[0], self.padding, self.strides[0])\r\n",
        "        return input_shape[0], conv_len, input_shape[2]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzIeXZ2OLfSd"
      },
      "source": [
        "class Dense(Layer):\r\n",
        "    def __init__(self, units=32, activation='relu', **kwargs):\r\n",
        "        super(Dense, self).__init__(**kwargs)\r\n",
        "        self.units = units\r\n",
        "        self.b = None  # b is initialized in build\r\n",
        "        self.w = None  # w is initialized in build\r\n",
        "        if activation is not None:\r\n",
        "            self.activation = activations.get(activation)\r\n",
        "        else:\r\n",
        "            self.activation = None\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.b = self.add_weight(\r\n",
        "            shape=(self.units,),\r\n",
        "            initializer=tf.keras.initializers.zeros(),\r\n",
        "            trainable=True,\r\n",
        "            dtype='float32'\r\n",
        "        )\r\n",
        "        self.w = self.add_weight(\r\n",
        "            shape=(input_shape[-1], self.units),\r\n",
        "            initializer=tf.keras.initializers.random_normal(),\r\n",
        "            trainable=True,\r\n",
        "            dtype='float32'\r\n",
        "        )\r\n",
        "        super(Dense, self).build(input_shape)\r\n",
        "\r\n",
        "    def call(self, x, **kwargs):\r\n",
        "        y = K.dot(x, self.w) + self.b\r\n",
        "        if self.activation is not None:\r\n",
        "            y = self.activation(y)\r\n",
        "        return y\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        return input_shape[0], self.units\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwu3NR_QL6Xj"
      },
      "source": [
        "class Conv2D(Layer):\r\n",
        "    bias: Union[Union[PartitionedVariable, ShardedVariable, Conv2D], Any]\r\n",
        "    kernel: Union[Union[PartitionedVariable, ShardedVariable, Conv2D], Any]\r\n",
        "\r\n",
        "    def __init__(self, filters=32, strides=(1, 1), padding='valid', activation='relu', dilation_rate=(1, 1), batch_size=1, **kwargs):\r\n",
        "\r\n",
        "        self.filters = filters\r\n",
        "        self.bias = None\r\n",
        "        self.kernel_size = (3, 3)\r\n",
        "        self.kernel = None\r\n",
        "        self.strides = strides\r\n",
        "        self.padding = padding\r\n",
        "        if activation is not None:\r\n",
        "            self.activation = activations.get(activation)\r\n",
        "        else:\r\n",
        "            self.activation = None\r\n",
        "        self.dilation_rate = dilation_rate\r\n",
        "        self.batch_size = batch_size\r\n",
        "        if K.image_data_format() == 'channels_first':\r\n",
        "            self.channel_axis = 0\r\n",
        "        else:\r\n",
        "            self.channel_axis = -1\r\n",
        "        super(Conv2D, self).__init__(**kwargs)\r\n",
        "\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        super(Conv2D, self).build(input_shape)\r\n",
        "        kernel_shape = self.kernel_size + (input_shape[self.channel_axis], self.filters)\r\n",
        "        self.bias = self.add_weight(name='bias',\r\n",
        "                                    shape=(self.filters,),\r\n",
        "                                    dtype='float32',\r\n",
        "                                    initializer=tf.zeros_initializer(),\r\n",
        "                                    trainable=True)\r\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\r\n",
        "                                      initializer=tf.keras.initializers.GlorotUniform(),\r\n",
        "                                      trainable=True)\r\n",
        "\r\n",
        "    def call(self, x, **kwargs):\r\n",
        "        y = tf.keras.backend.conv2d(x, self.kernel)\r\n",
        "        if self.activation is not None:\r\n",
        "            y = self.activation(y)\r\n",
        "        return y\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        batch_size = input_shape[0]\r\n",
        "        convX = conv_utils.conv_output_length(\r\n",
        "            input_shape[1],\r\n",
        "            self.kernel_size[0],\r\n",
        "            padding=self.padding,\r\n",
        "            stride=self.strides[0],\r\n",
        "            dilation=self.dilation_rate[0]\r\n",
        "        )\r\n",
        "        convY = conv_utils.conv_output_length(\r\n",
        "            input_shape[2],\r\n",
        "            self.kernel_size[1],\r\n",
        "            padding=self.padding,\r\n",
        "            stride=self.strides[1],\r\n",
        "            dilation=self.dilation_rate[1]\r\n",
        "        )\r\n",
        "        return batch_size, convX, convY, self.filters"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpUc9LJO5l4I"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MEv26XxqmxB"
      },
      "source": [
        "import keras\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow import keras\r\n",
        "from keras import layers\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.layers import Input, Dropout\r\n",
        "\r\n",
        "model = model_first = keras.Sequential([\r\n",
        "\r\n",
        "    keras.layers.Input(shape=(image_width, image_height, 1)),\r\n",
        "    Conv2D(filters = 32),\r\n",
        "    Conv2D(filters = 32),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "    layers.BatchNormalization(),\r\n",
        "    layers.Dropout(0.4),\r\n",
        "\r\n",
        "    Conv2D(filters = 64),\r\n",
        "    Conv2D(filters = 64),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "    layers.BatchNormalization(),\r\n",
        "    layers.Dropout(0.4),\r\n",
        "    \r\n",
        "    Conv2D(filters = 128),\r\n",
        "    Conv2D(filters = 128),\r\n",
        "    Conv2D(filters = 128),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "    layers.BatchNormalization(),\r\n",
        "    layers.Dropout(0.4),\r\n",
        "\r\n",
        "    Conv2D(filters = 256),\r\n",
        "    Conv2D(filters = 256),\r\n",
        "    Conv2D(filters = 256),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "    layers.BatchNormalization(),\r\n",
        "    layers.Dropout(0.4),\r\n",
        "\r\n",
        "    Conv2D(filters = 256),\r\n",
        "    Conv2D(filters = 256),\r\n",
        "    Conv2D(filters = 256),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "    layers.BatchNormalization(),\r\n",
        "    layers.Dropout(0.4),\r\n",
        "    \r\n",
        "    layers.Flatten(),\r\n",
        "    Dense(units=512),\r\n",
        "    layers.Dropout(0.5),\r\n",
        "\r\n",
        "    Dense(units=512),\r\n",
        "    layers.Dropout(0.5),    \r\n",
        "\r\n",
        "    Dense(units=1, activation = \"sigmoid\"),\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "model_first.compile(optimizer='rmsprop',\r\n",
        "              loss='binary_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model.compile(optimizer='rmsprop',\r\n",
        "              loss='binary_crossentropy',\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWScRVvt70a5",
        "outputId": "f0d9edba-4ddb-4494-9702-544c6223a88f"
      },
      "source": [
        "patience = 5\r\n",
        "\r\n",
        "cb = keras.callbacks.EarlyStopping(\r\n",
        "    monitor='val_loss', min_delta=0, patience=patience, verbose=0, mode='auto',\r\n",
        "    baseline=None, restore_best_weights=True\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "history =model_first.fit(train_generator,\r\n",
        "                  epochs=1000, #we stop training the model with early stopping\r\n",
        "                  validation_data=validation_generator,\r\n",
        "                  callbacks=[cb])\r\n",
        "\r\n",
        "\r\n",
        "#retrain on the whole dataset for the same epochs as before\r\n",
        "epochs = (len (history.history['loss']) - patience)\r\n",
        "\r\n",
        "history =model.fit(trainAll_generator,\r\n",
        "                  epochs = epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_26/bias:0', 'conv2d_27/bias:0', 'conv2d_28/bias:0', 'conv2d_29/bias:0', 'conv2d_30/bias:0', 'conv2d_31/bias:0', 'conv2d_32/bias:0', 'conv2d_33/bias:0', 'conv2d_34/bias:0', 'conv2d_35/bias:0', 'conv2d_36/bias:0', 'conv2d_37/bias:0', 'conv2d_38/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_26/bias:0', 'conv2d_27/bias:0', 'conv2d_28/bias:0', 'conv2d_29/bias:0', 'conv2d_30/bias:0', 'conv2d_31/bias:0', 'conv2d_32/bias:0', 'conv2d_33/bias:0', 'conv2d_34/bias:0', 'conv2d_35/bias:0', 'conv2d_36/bias:0', 'conv2d_37/bias:0', 'conv2d_38/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_26/bias:0', 'conv2d_27/bias:0', 'conv2d_28/bias:0', 'conv2d_29/bias:0', 'conv2d_30/bias:0', 'conv2d_31/bias:0', 'conv2d_32/bias:0', 'conv2d_33/bias:0', 'conv2d_34/bias:0', 'conv2d_35/bias:0', 'conv2d_36/bias:0', 'conv2d_37/bias:0', 'conv2d_38/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_26/bias:0', 'conv2d_27/bias:0', 'conv2d_28/bias:0', 'conv2d_29/bias:0', 'conv2d_30/bias:0', 'conv2d_31/bias:0', 'conv2d_32/bias:0', 'conv2d_33/bias:0', 'conv2d_34/bias:0', 'conv2d_35/bias:0', 'conv2d_36/bias:0', 'conv2d_37/bias:0', 'conv2d_38/bias:0'] when minimizing the loss.\n",
            " 7/17 [===========>..................] - ETA: 7:46 - loss: 0.9767 - accuracy: 0.5335"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}