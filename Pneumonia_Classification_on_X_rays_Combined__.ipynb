{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Pneumonia_Classification_on_X-rays_Combined.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4oM-lVBu3K7",
        "outputId": "a78cca96-e031-422e-93d0-1a0677b4fe24"
      },
      "source": [
        "# Only for colab\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cffsxgv-qmww"
      },
      "source": [
        "import os, shutil\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import preprocessing\n",
        "import os, shutil, glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "945vKEfGqmw4"
      },
      "source": [
        "## Load data\n",
        "### Make sure this file and original dataset in the same directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8RzOG1Mqmw4"
      },
      "source": [
        "# Directory in which we store our dataset\n",
        "\n",
        "# Old dataset path\n",
        "original_dataset_dir = 'drive/My Drive/lung/filtered'\n",
        "\n",
        "# New dataset path\n",
        "base_dir = 'drive/My Drive/lung/new_set'\n",
        "# Create a new directory\n",
        "#os.mkdir(base_dir)   # comment this line it if you've run it"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iA_JZwRqmw5"
      },
      "source": [
        "# Directory in which we store training, validation\n",
        "# And Create directories of training and validation for normal and pneumonia respectively \n",
        "\n",
        "train_dir = os.path.join(base_dir,'train')\n",
        "#os.mkdir(train_dir)               # comment this line it if you've run it\n",
        "\n",
        "validation_dir = os.path.join(base_dir,'validation')\n",
        "#os.mkdir(validation_dir)          # comment this line it if you've run it\n",
        "\n",
        "\n",
        "train_normal_dir = os.path.join(train_dir, 'NORMAL')\n",
        "#os.mkdir(train_normal_dir)        # comment this line it if you've run it\n",
        "\n",
        "train_pneumonia_dir = os.path.join(train_dir, 'PNEUMONIA')\n",
        "#os.mkdir(train_pneumonia_dir)     # comment this line it if you've run it\n",
        "\n",
        "validation_normal_dir = os.path.join(validation_dir, 'NORMAL')\n",
        "#os.mkdir(validation_normal_dir)   # ccomment this line it if you've run it\n",
        "\n",
        "validation_pneumonia_dir = os.path.join(validation_dir, 'PNEUMONIA')\n",
        "#os.mkdir(validation_pneumonia_dir)# comment this line it if you've run it"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmzsUwNZqmw5"
      },
      "source": [
        "# All jpeg data from old directory\n",
        "imageList_normal = glob.glob(os.path.join( (original_dataset_dir + '/NORMAL/'), '*.jpeg'))\n",
        "imageList_pneumonia = glob.glob(os.path.join( (original_dataset_dir + '/PNEUMONIA/'), '*.jpeg'))\n",
        "\n",
        "# All jpeg data from new directories\n",
        "imageList_train_normal = glob.glob(os.path.join( (base_dir + '/train/NORMAL/'), '*.jpeg'))\n",
        "imageList_val_normal = glob.glob(os.path.join( (base_dir + '/validation/NORMAL/'), '*.jpeg'))\n",
        "\n",
        "imageList_train_pneumonia = glob.glob(os.path.join(  (base_dir + '/train/PNEUMONIA/'), '*.jpeg'))\n",
        "imageList_val_pneumonia = glob.glob(os.path.join(  (base_dir + '/validation/PNEUMONIA/'), '*.jpeg'))\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ixPT28qmw5"
      },
      "source": [
        "def is_file(fname_from,imageList):\n",
        "    check=False\n",
        "    for f in imageList: \n",
        "        fname_to = os.path.basename(f)\n",
        "        if(fname_to == fname_from):\n",
        "            check=True\n",
        "    return check"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRuQXIs9qmw6"
      },
      "source": [
        "\n",
        "normal_cases_dir =   (original_dataset_dir + '/NORMAL/')    # Old normal path\n",
        "pneumonia_cases_dir = (original_dataset_dir + '/PNEUMONIA/')# Old pneumonia path\n",
        "\n",
        "\n",
        "LENTH_DATASET=len(imageList_normal)                # total normal images (as the same as pneumonia images)\n",
        "\n",
        "split_train_percent=int(LENTH_DATASET*0.8)   #0.8 for train data\n",
        "split_val_percent=int(LENTH_DATASET-split_train_percent)  #0.2 for train data\n",
        "\n",
        "count=0\n",
        "\n",
        "# Split normal data into 80% train and 20% validation\n",
        "for f in imageList_normal:\n",
        "    count+=1    \n",
        "    fname = os.path.basename(f)\n",
        "    if(count<=split_train_percent and is_file(fname,imageList_train_normal)==False):\n",
        "        src = os.path.join(normal_cases_dir, fname)\n",
        "        dst = os.path.join(train_normal_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    elif(count>split_train_percent and is_file(fname,imageList_val_normal)==False):\n",
        "        src = os.path.join(normal_cases_dir, fname)\n",
        "        dst = os.path.join(validation_normal_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    \n",
        "count=0\n",
        "\n",
        "# Split pneumonia data into 80% train and 20% validation\n",
        "for f in imageList_pneumonia:\n",
        "    count+=1    \n",
        "    fname = os.path.basename(f)\n",
        "    if(count<=split_train_percent and is_file(fname,imageList_train_pneumonia)==False):\n",
        "        src = os.path. join(pneumonia_cases_dir, fname)\n",
        "        dst = os.path.join(train_pneumonia_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    elif(count>split_train_percent and is_file(fname,imageList_val_pneumonia)==False):\n",
        "        src = os.path.join(pneumonia_cases_dir, fname)\n",
        "        dst = os.path.join(validation_pneumonia_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDyDg-_bqmw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7821e1f-f53e-453a-9079-439a10c77203"
      },
      "source": [
        "print('total training normal images:', len(os.listdir(train_normal_dir)))\n",
        "print('total training pneumonia images:', len(os.listdir(train_pneumonia_dir)))\n",
        "\n",
        "print('total training normal images:', len(os.listdir(validation_normal_dir)))\n",
        "print('total training pneumonia images:', len(os.listdir(validation_pneumonia_dir)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training normal images: 1341\n",
            "total training pneumonia images: 1351\n",
            "total training normal images: 556\n",
            "total training pneumonia images: 548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "facOl3UPqmw8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "233592e6-3d99-4bc6-a72f-e339bba635f8"
      },
      "source": [
        "# Structure of directory\n",
        "\n",
        "BRANCH = '├─'\n",
        "LAST_BRANCH = '└─'\n",
        "TAB = '│  '\n",
        "EMPTY_TAB = '   '\n",
        "\n",
        "\n",
        "def get_dir_list(path, placeholder=''):\n",
        "    folder_list = [folder for folder in os.listdir(path) if os.path.isdir(os.path.join(path, folder))]\n",
        "    file_list = [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
        "    result = ''\n",
        "    for folder in folder_list[:-1]:\n",
        "        result += placeholder + BRANCH + folder + '\\n'\n",
        "        result += get_dir_list(os.path.join(path, folder), placeholder + TAB)\n",
        "    if folder_list:\n",
        "        result += placeholder + (BRANCH if file_list else LAST_BRANCH) + folder_list[-1] + '\\n'\n",
        "        result += get_dir_list(os.path.join(path, folder_list[-1]), placeholder + (TAB if file_list else EMPTY_TAB))\n",
        "    return result\n",
        "if __name__ == '__main__':\n",
        "    print(get_dir_list('./new_set'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-80189b2da29c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_dir_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./new_set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-80189b2da29c>\u001b[0m in \u001b[0;36mget_dir_list\u001b[0;34m(path, placeholder)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dir_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfolder_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './new_set'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiN8ungSqmw8"
      },
      "source": [
        "## Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMMxaoV5qmw8"
      },
      "source": [
        "from glob import glob\n",
        "import cv2\n",
        "print(\"No Pneumonia\")\n",
        "multipleImages = glob('./new_set/train/NORMAL/**')\n",
        "i_ = 0\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "for l in multipleImages[:10]:\n",
        "    im = cv2.imread(l)\n",
        "    im = cv2.resize(im, (128, 128)) \n",
        "    plt.subplot(5, 5, i_+1) \n",
        "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
        "    i_ += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwOMcen9qmw9"
      },
      "source": [
        "print(\"Pneumonia\")\n",
        "multipleImages = glob('./new_set/train/PNEUMONIA/**')\n",
        "i_ = 0\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "for l in multipleImages[:10]:\n",
        "    im = cv2.imread(l)\n",
        "    im = cv2.resize(im, (128, 128)) \n",
        "    plt.subplot(5, 5, i_+1) \n",
        "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
        "    i_ += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlDC2Kzuqmw-"
      },
      "source": [
        "## DataGenerator and Image augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUkQcYcpsgoM",
        "outputId": "475501e8-c105-483e-9635-65f1e9d5e5d2"
      },
      "source": [
        "!pip install nbimporter"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nbimporter in /usr/local/lib/python3.6/dist-packages (0.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf7oafdhqHRV"
      },
      "source": [
        "import nbimporter   # install it for load ipyb inJupyter\r\n",
        "#from my_dataGenerator import DataGenerator  # invoke custom data generator class\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import cv2\r\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmYlXmdrsWx9"
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\r\n",
        "    'Generates data for Keras'\r\n",
        "    def __init__(self, list_IDs, labels, batch_size=32, dim=(224,224), n_channels=1,\r\n",
        "                 n_classes=2, shuffle=True,color_mode='grayscale'):\r\n",
        "        'Initialization'\r\n",
        "        self.dim = dim\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.labels = labels\r\n",
        "        self.list_IDs = list_IDs\r\n",
        "        self.n_channels = n_channels\r\n",
        "        self.n_classes = n_classes\r\n",
        "        self.shuffle = shuffle\r\n",
        "        self.color_mode=color_mode,\r\n",
        "        self.on_epoch_end()\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        'Denotes the number of batches per epoch'\r\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        'Generate one batch of data'\r\n",
        "        # Generate indexes of the batch\r\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n",
        "\r\n",
        "        # Find list of IDs\r\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\r\n",
        "\r\n",
        "        # Generate data\r\n",
        "        X, y = self.__data_generation(list_IDs_temp)\r\n",
        "\r\n",
        "        return X, y\r\n",
        "\r\n",
        "    def on_epoch_end(self):\r\n",
        "        'Updates indexes after each epoch'\r\n",
        "        self.indexes = np.arange(len(self.list_IDs))\r\n",
        "        if self.shuffle == True:\r\n",
        "            np.random.shuffle(self.indexes)\r\n",
        "\r\n",
        "    def __data_generation(self, list_IDs_temp):\r\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\r\n",
        "        # Initialization\r\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\r\n",
        "        y = np.empty((self.batch_size), dtype=int)\r\n",
        "\r\n",
        "        # Generate data\r\n",
        "        for i, ID in enumerate(list_IDs_temp):\r\n",
        "            # Store sample\r\n",
        "            X[i,] = self._load_grayscale_image(ID)\r\n",
        "            \r\n",
        "            # Store class\r\n",
        "            y[i] = self.labels[ID]\r\n",
        "\r\n",
        "        return X,y \r\n",
        "\r\n",
        "    def _load_grayscale_image(self, image_path):\r\n",
        "        \"\"\"Load grayscale image\r\n",
        "\r\n",
        "        :param image_path: path to image to load\r\n",
        "        :return: loaded image\r\n",
        "        \"\"\"\r\n",
        "        img = cv2.imread(image_path)\r\n",
        "        img = cv2.resize(img, (224,224))\r\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\r\n",
        "        img=img.astype('float')\r\n",
        "        img=img.reshape(224,224,1) \r\n",
        "        img = img / 255\r\n",
        "        return img"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73cE02kkqTMq"
      },
      "source": [
        "train_0=imageList_train_normal+imageList_val_normal   # all normal images path\r\n",
        "lable_train_0=[]\r\n",
        "    \r\n",
        "lable_train_0 = dict.fromkeys(train_0, 0)             # set normal as 0\r\n",
        "\r\n",
        "\r\n",
        "train_1=imageList_train_pneumonia+imageList_val_pneumonia  # all pneumonia  images path\r\n",
        "lable_train_1=[]\r\n",
        "   \r\n",
        "lable_train_1 = dict.fromkeys(train_1, 1)      # set pneumonia as 1\r\n",
        "\r\n",
        "lables = {**lable_train_0, **lable_train_1}    # Store both labels into a dictionary \r\n",
        "\r\n",
        "# lables format as {'./new_set/train/NORMAL\\\\IM-0115-0001.jpeg': 0, './new_set/train/NORMAL\\\\NORMAL2-IM-0533-0001.jpeg': 1, ...}\r\n",
        "\r\n",
        "\r\n",
        "from collections import defaultdict  # install it for easy operate dictionary array\r\n",
        "\r\n",
        "train_d=imageList_train_normal+imageList_train_pneumonia       # all train images path\r\n",
        "val_d=imageList_val_normal+imageList_val_pneumonia             # all val images path\r\n",
        "all_d=imageList_val_normal+imageList_val_pneumonia+imageList_train_normal+imageList_train_pneumonia# all val images path\r\n",
        "\r\n",
        "partition = defaultdict(list)         #init a dictionary\r\n",
        "for j in train_d:    \r\n",
        "    partition['train'].append(j)      # Train as key and all train images path as value\r\n",
        "for j in val_d:    \r\n",
        "    partition['validation'].append(j) # validation as key and all val images path as value  \r\n",
        "for j in all_d:    \r\n",
        "    partition['all'].append(j) # validation as key and all val images path as value  \r\n",
        "  \r\n",
        "#  partition format as  {'train': ['./new_set/train/NORMAL\\\\IM-0115-0001.jpeg', ..], \r\n",
        "# 'validation': ['./new_set/train/NORMAL\\\\NORMAL2-IM-0533-0001.jpeg']}"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14l54pH5qmw-"
      },
      "source": [
        "# Parameters\n",
        "params = {'dim': (224,224),\n",
        "          'batch_size': 32,\n",
        "          'n_classes': 2,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Datasets\n",
        "partition = partition   # path as IDs\n",
        "labels = lables         # Labels\n",
        "\n",
        "# Call DataGenerator function from custom class \n",
        "training_generator = DataGenerator(partition['train'], labels, **params)\n",
        "validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
        "all_generator = DataGenerator(partition['all'], labels, **params)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cabAP9zPKWdG"
      },
      "source": [
        "# Custom Layers\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g_gXj7-MEGS"
      },
      "source": [
        "from math import ceil\r\n",
        "from keras.utils import conv_utils\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import activations\r\n",
        "from keras import backend as K\r\n",
        "from typing import Any, Union\r\n",
        "import keras.backend as K\r\n",
        "from keras import activations\r\n",
        "from tensorflow.python.distribute.sharded_variable import ShardedVariable\r\n",
        "from tensorflow.python.ops.variables import PartitionedVariable"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDLyhTPEKa7D"
      },
      "source": [
        "class MaxPooling(Layer):\r\n",
        "    def __init__(self, pool_size=(2, 2), strides=None, padding='VALID', **kwargs):\r\n",
        "        super(MaxPooling, self).__init__(**kwargs)\r\n",
        "        self.pool_size = pool_size\r\n",
        "        if strides is not None:\r\n",
        "            self.strides = strides\r\n",
        "        else:\r\n",
        "            self.strides = pool_size\r\n",
        "        self.padding = padding\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        super(MaxPooling, self).build(input_shape)\r\n",
        "\r\n",
        "    def call(self, x, **kwargs):\r\n",
        "        y = tf.nn.max_pool2d(x, self.pool_size, self.strides, self.padding)\r\n",
        "        return y\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        # conv_len = conv_utils.conv_output_length(input_shape[1], self.pool_size[0], self.padding, self.strides[0])\r\n",
        "        rows = ceil(float(input_shape[1]) / float(self.strides[0]))\r\n",
        "        cols = ceil(float(input_shape[2]) / float(self.strides[1]))\r\n",
        "        return input_shape[0], rows, cols, input_shape[-1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzIeXZ2OLfSd"
      },
      "source": [
        "class Dense(Layer):\r\n",
        "    def __init__(self, units=32, activation='relu', **kwargs):\r\n",
        "        super(Dense, self).__init__(**kwargs)\r\n",
        "        self.units = units\r\n",
        "        self.b = None  # b is initialized in build\r\n",
        "        self.w = None  # w is initialized in build\r\n",
        "        if activation is not None:\r\n",
        "            self.activation = activations.get(activation)\r\n",
        "        else:\r\n",
        "            self.activation = None\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.b = self.add_weight(\r\n",
        "            name=\"dense_weight_b\",\r\n",
        "            shape=(self.units,),\r\n",
        "            initializer=tf.keras.initializers.zeros(),\r\n",
        "            trainable=True,\r\n",
        "            dtype='float32'\r\n",
        "        )\r\n",
        "        self.w = self.add_weight(\r\n",
        "            name=\"dense_weight_w\",\r\n",
        "            shape=(input_shape[-1], self.units),\r\n",
        "            initializer=tf.keras.initializers.random_normal(),\r\n",
        "            trainable=True,\r\n",
        "            dtype='float32'\r\n",
        "        )\r\n",
        "        super(Dense, self).build(input_shape)\r\n",
        "\r\n",
        "    def call(self, x, **kwargs):\r\n",
        "        y = K.dot(x, self.w) + self.b\r\n",
        "        if self.activation is not None:\r\n",
        "            y = self.activation(y)\r\n",
        "        return y\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        return input_shape[0], self.units\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwu3NR_QL6Xj"
      },
      "source": [
        "class Conv2D(Layer):\r\n",
        "    bias: Union[Union[PartitionedVariable, ShardedVariable, Conv2D], Any]\r\n",
        "    kernel: Union[Union[PartitionedVariable, ShardedVariable, Conv2D], Any]\r\n",
        "\r\n",
        "    def __init__(self, filters=32, strides=(1, 1), padding='valid', activation='relu', dilation_rate=(1, 1), batch_size=1, **kwargs):\r\n",
        "\r\n",
        "        self.filters = filters\r\n",
        "        self.bias = None\r\n",
        "        self.kernel_size = (3, 3)\r\n",
        "        self.kernel = None\r\n",
        "        self.strides = strides\r\n",
        "        self.padding = padding\r\n",
        "        if activation is not None:\r\n",
        "            self.activation = activations.get(activation)\r\n",
        "        else:\r\n",
        "            self.activation = None\r\n",
        "        self.dilation_rate = dilation_rate\r\n",
        "        self.batch_size = batch_size\r\n",
        "        if K.image_data_format() == 'channels_first':\r\n",
        "            self.channel_axis = 0\r\n",
        "        else:\r\n",
        "            self.channel_axis = -1\r\n",
        "        super(Conv2D, self).__init__(**kwargs)\r\n",
        "\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        super(Conv2D, self).build(input_shape)\r\n",
        "        kernel_shape = self.kernel_size + (input_shape[self.channel_axis], self.filters)\r\n",
        "        self.bias = self.add_weight(name='bias',\r\n",
        "                                    shape=(self.filters,),\r\n",
        "                                    dtype='float32',\r\n",
        "                                    initializer=tf.zeros_initializer(),\r\n",
        "                                    trainable=True)\r\n",
        "        self.kernel = self.add_weight(name='kernel',\r\n",
        "                                      shape=kernel_shape,\r\n",
        "                                      initializer=tf.keras.initializers.GlorotUniform(),\r\n",
        "                                      trainable=True)\r\n",
        "\r\n",
        "    def call(self, x, **kwargs):\r\n",
        "        y = tf.keras.backend.conv2d(x, self.kernel)\r\n",
        "        y = K.bias_add(y, self.bias)\r\n",
        "        if self.activation is not None:\r\n",
        "            y = self.activation(y)\r\n",
        "        return y\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        batch_size = input_shape[0]\r\n",
        "        convX = conv_utils.conv_output_length(\r\n",
        "            input_shape[1],\r\n",
        "            self.kernel_size[0],\r\n",
        "            padding=self.padding,\r\n",
        "            stride=self.strides[0],\r\n",
        "            dilation=self.dilation_rate[0]\r\n",
        "        )\r\n",
        "        convY = conv_utils.conv_output_length(\r\n",
        "            input_shape[2],\r\n",
        "            self.kernel_size[1],\r\n",
        "            padding=self.padding,\r\n",
        "            stride=self.strides[1],\r\n",
        "            dilation=self.dilation_rate[1]\r\n",
        "        )\r\n",
        "        return batch_size, convX, convY, self.filters"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpUc9LJO5l4I"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MEv26XxqmxB"
      },
      "source": [
        "import keras\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow import keras\r\n",
        "from keras import layers\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.layers import Input, Dropout, SpatialDropout2D, experimental\r\n",
        "model = model_2 = keras.Sequential([\r\n",
        "                          \r\n",
        "    #experimental.preprocessing.RandomRotation(30),\r\n",
        "    #experimental.preprocessing.RandomZoom(height_factor = (-0.2, 0.2) , width_factor = (-0.2, 0.2)),\r\n",
        "    #experimental.preprocessing.RandomTranslation(height_factor = (-0.2, 0.2) , width_factor = (-0.2, 0.2)),\r\n",
        "    #experimental.preprocessing.RandomFlip( mode=\"horizontal\"),\r\n",
        "\r\n",
        "    keras.layers.Input(shape=(224, 224, 1)),\r\n",
        "    Conv2D(filters = 64, padding=\"zero\", strides = (3,3)),\r\n",
        "    SpatialDropout2D(0.2),\r\n",
        "    BatchNormalization(),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "\r\n",
        "\r\n",
        "    Conv2D(filters = 128, padding=\"zero\", strides = (3,3)),\r\n",
        "    SpatialDropout2D(0.2),\r\n",
        "    BatchNormalization(),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "\r\n",
        "\r\n",
        "    Conv2D(filters = 256, padding=\"zero\", strides = (3,3)),\r\n",
        "    SpatialDropout2D(0.2),\r\n",
        "    BatchNormalization(),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "\r\n",
        "\r\n",
        "    Conv2D(filters = 512, padding=\"zero\", strides = (3,3)),\r\n",
        "    SpatialDropout2D(0.2),\r\n",
        "    BatchNormalization(),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "\r\n",
        "\r\n",
        "    Conv2D(filters = 1024, padding=\"zero\", strides = (3,3)),\r\n",
        "    SpatialDropout2D(0.2),\r\n",
        "    BatchNormalization(),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "\r\n",
        "\r\n",
        "    layers.Flatten(),\r\n",
        "    Dense(units=2048),\r\n",
        "    BatchNormalization(),\r\n",
        "\r\n",
        "    Dense(units=1, activation = \"sigmoid\"),\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "model.compile(optimizer='rmsprop',\r\n",
        "              loss='binary_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "RWScRVvt70a5",
        "outputId": "939e2801-990f-4d5b-8119-68cc48000c78"
      },
      "source": [
        "patience = 5\r\n",
        "\r\n",
        "cb = keras.callbacks.EarlyStopping(\r\n",
        "    monitor='val_loss', min_delta=0, patience=patience, verbose=0, mode='auto',\r\n",
        "    baseline=None, restore_best_weights=False\r\n",
        ")\r\n",
        "\r\n",
        "history = model_2.fit(training_generator,\r\n",
        "                  epochs=1000, #we stop training the model with early stopping\r\n",
        "                  validation_data=validation_generator,\r\n",
        "                  callbacks=[cb])\r\n",
        "\r\n",
        "\r\n",
        "model_2.save_weights('model_first.h5')\r\n",
        "\r\n",
        "#retrain on the whole dataset for the same epochs as before\r\n",
        "model.load_weights('def.h5')\r\n",
        "epochs = (len (history.history['loss']) - patience)\r\n",
        "\r\n",
        "history = model.fit(all_generator,\r\n",
        "                  epochs = epochs)\r\n",
        "\r\n",
        "\r\n",
        "model.save_weights('model.h5')\r\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "84/84 [==============================] - 81s 948ms/step - loss: 1.9516 - accuracy: 0.8660 - val_loss: 16.3873 - val_accuracy: 0.4945\n",
            "Epoch 2/1000\n",
            "84/84 [==============================] - 79s 940ms/step - loss: 0.2923 - accuracy: 0.9363 - val_loss: 17.6986 - val_accuracy: 0.4991\n",
            "Epoch 3/1000\n",
            "84/84 [==============================] - 78s 926ms/step - loss: 0.1600 - accuracy: 0.9530 - val_loss: 16.9835 - val_accuracy: 0.4963\n",
            "Epoch 4/1000\n",
            "84/84 [==============================] - 77s 914ms/step - loss: 0.0961 - accuracy: 0.9687 - val_loss: 2.1513 - val_accuracy: 0.5046\n",
            "Epoch 5/1000\n",
            "84/84 [==============================] - 77s 913ms/step - loss: 0.0798 - accuracy: 0.9747 - val_loss: 3.1630 - val_accuracy: 0.4972\n",
            "Epoch 6/1000\n",
            " 6/84 [=>............................] - ETA: 51s - loss: 0.0156 - accuracy: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-9d815f56da87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#we stop training the model with early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                   callbacks=[cb])\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkYYuYgQqmw_"
      },
      "source": [
        "# Task 3 - Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-YVkZvbqmw_"
      },
      "source": [
        "# Display how many test data respect to normal and pneumonia\n",
        "test_dir = './encoded'\n",
        "\n",
        "test_normal_dir = os.path.join(test_dir, 'NORMAL')\n",
        "test_pneumonia_dir = os.path.join(test_dir, 'PNEUMONIA')\n",
        "\n",
        "print('total test normal images:', len(os.listdir(test_normal_dir)))\n",
        "print('total test pneumonia images:', len(os.listdir(test_pneumonia_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjSj343zqmxA"
      },
      "source": [
        "# Read the test text file and store into list\n",
        "# And split into x_test and y_test for labels\n",
        "test_dir = './encoded'\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['NORMAL', 'PNEUMONIA']:    \n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            tex = []\n",
        "            for line in f:\n",
        "                row = line.split()\n",
        "                for r in row:\n",
        "                    r1 = float(r)  # Convert string to float\n",
        "                    tex.append(r1) # (50176)\n",
        "                                               \n",
        "            texts.append(tex)      # (51*50176)\n",
        "            if label_type == 'NORMAL':\n",
        "                labels.append(0)   # Labled Normal to 0     \n",
        "            else:\n",
        "                labels.append(1)   # Labled PNEUMONIA to 1         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeqYQakpqmxA"
      },
      "source": [
        "# Convert list to np array in order to reshape\n",
        "x_test=np.array(texts)\n",
        "y_test=np.array(labels)\n",
        "print(x_test.shape)\n",
        "\n",
        "# Reshape as the same format as train\n",
        "x_test=x_test.reshape(51,224,224,1) \n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "y7qOZaUeqmxB"
      },
      "source": [
        "model.load_weights('model.h5')\n",
        "# Choose one of them\n",
        "test_iterator = test_datagen.flow(x_test, y_test, batch_size=batch)\n",
        "test_loss, test_acc =model.evaluate(x_test, y_test, steps=46)\n",
        "\n",
        "print('test acc:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}