{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Pneumonia_Classification_on_X-rays_Combined.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4oM-lVBu3K7",
        "outputId": "266b7b9a-79ea-4787-c124-86bb5db5dbad"
      },
      "source": [
        "# Only for colab\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cffsxgv-qmww"
      },
      "source": [
        "import os, shutil\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import preprocessing\n",
        "import os, shutil, glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "945vKEfGqmw4"
      },
      "source": [
        "## Load data\n",
        "### Make sure this file and original dataset in the same directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8RzOG1Mqmw4"
      },
      "source": [
        "# Directory in which we store our dataset\n",
        "\n",
        "# Old dataset path\n",
        "original_dataset_dir =  'drive/My Drive/lung/filtered'\n",
        "\n",
        "# New dataset path\n",
        "base_dir = 'drive/My Drive/lung/new_set'\n",
        "# Create a new directory\n",
        "\n",
        "# New dataset path\n",
        "test_dir = 'drive/My Drive/lung/encoded'\n",
        "\n",
        "#os.mkdir(base_dir)   # comment this line it if you've run it\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iA_JZwRqmw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f8f94b-b0bf-4c4e-b5c8-3c3d3a13e8ba"
      },
      "source": [
        "# Directory in which we store training, validation\n",
        "# And Create directories of training and validation for normal and pneumonia respectively \n",
        "\n",
        "train_dir = os.path.join(base_dir,'train')\n",
        "#os.mkdir(train_dir)               # comment this line it if you've run it\n",
        "\n",
        "validation_dir = os.path.join(base_dir,'validation')\n",
        "#os.mkdir(validation_dir)          # comment this line it if you've run it\n",
        "\n",
        "\n",
        "train_normal_dir = os.path.join(train_dir, 'NORMAL')\n",
        "#os.mkdir(train_normal_dir)        # comment this line it if you've run it\n",
        "\n",
        "train_pneumonia_dir = os.path.join(train_dir, 'PNEUMONIA')\n",
        "#os.mkdir(train_pneumonia_dir)     # comment this line it if you've run it\n",
        "\n",
        "validation_normal_dir = os.path.join(validation_dir, 'NORMAL')\n",
        "#os.mkdir(validation_normal_dir)   # ccomment this line it if you've run it\n",
        "\n",
        "validation_pneumonia_dir = os.path.join(validation_dir, 'PNEUMONIA')\n",
        "#os.mkdir(validation_pneumonia_dir)# comment this line it if you've run it\n",
        "\n",
        "test_normal_dir = os.path.join(test_dir, 'NORMAL')\n",
        "#os.mkdir(validation_normal_dir)   # ccomment this line it if you've run it\n",
        "\n",
        "test_pneumonia_dir = os.path.join(test_dir, 'PNEUMONIA')\n",
        "#os.mkdir(validation_pneumonia_dir)# comment this line it if you've run it\n",
        "\n",
        "print(train_dir)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/lung/new_set/train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmzsUwNZqmw5"
      },
      "source": [
        "# All jpeg data from old directory\n",
        "imageList_normal = glob.glob(os.path.join( (original_dataset_dir + '/NORMAL/'), '*.jpeg'))\n",
        "imageList_pneumonia = glob.glob(os.path.join( (original_dataset_dir + '/PNEUMONIA/'), '*.jpeg'))\n",
        "\n",
        "# All jpeg data from new directories\n",
        "imageList_train_normal = glob.glob(os.path.join( (base_dir + '/train/NORMAL/'), '*.jpeg'))\n",
        "imageList_val_normal = glob.glob(os.path.join( (base_dir + '/validation/NORMAL/'), '*.jpeg'))\n",
        "\n",
        "imageList_train_pneumonia = glob.glob(os.path.join(  (base_dir + '/train/PNEUMONIA/'), '*.jpeg'))\n",
        "imageList_val_pneumonia = glob.glob(os.path.join(  (base_dir + '/validation/PNEUMONIA/'), '*.jpeg'))\n",
        "\n",
        "imageList_test_normal = glob.glob(os.path.join( './encoded/NORMAL/', '.txt'))\n",
        "imageList_test_pneumonia  = glob.glob(os.path.join( './encoded/PNEUMONIA/', '.txt'))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ixPT28qmw5"
      },
      "source": [
        "def is_file(fname_from,imageList):\n",
        "    check=False\n",
        "    for f in imageList: \n",
        "        fname_to = os.path.basename(f)\n",
        "        if(fname_to == fname_from):\n",
        "            check=True\n",
        "    return check"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRuQXIs9qmw6"
      },
      "source": [
        "# \\\\don't run this again// \n",
        "normal_cases_dir =   (original_dataset_dir + '/NORMAL/')    # Old normal path\n",
        "pneumonia_cases_dir = (original_dataset_dir + '/PNEUMONIA/')# Old pneumonia path\n",
        "\n",
        "\n",
        "LENTH_DATASET=len(imageList_normal)                # total normal images (as the same as pneumonia images)\n",
        "\n",
        "split_train_percent=int(LENTH_DATASET*0.8)   #0.8 for train data\n",
        "split_val_percent=int(LENTH_DATASET-split_train_percent)  #0.2 for train data\n",
        "\n",
        "count=0\n",
        "\n",
        "# Split normal data into 80% train and 20% validation\n",
        "for f in imageList_normal:\n",
        "    count+=1    \n",
        "    fname = os.path.basename(f)\n",
        "    if(count<=split_train_percent and is_file(fname,imageList_train_normal)==False):\n",
        "        src = os.path.join(normal_cases_dir, fname)\n",
        "        dst = os.path.join(train_normal_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    elif(count>split_train_percent and is_file(fname,imageList_val_normal)==False):\n",
        "        src = os.path.join(normal_cases_dir, fname)\n",
        "        dst = os.path.join(validation_normal_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    \n",
        "count=0\n",
        "\n",
        "# Split pneumonia data into 80% train and 20% validation\n",
        "for f in imageList_pneumonia:\n",
        "    count+=1    \n",
        "    fname = os.path.basename(f)\n",
        "    if(count<=split_train_percent and is_file(fname,imageList_train_pneumonia)==False):\n",
        "        src = os.path. join(pneumonia_cases_dir, fname)\n",
        "        dst = os.path.join(train_pneumonia_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    elif(count>split_train_percent and is_file(fname,imageList_val_pneumonia)==False):\n",
        "        src = os.path.join(pneumonia_cases_dir, fname)\n",
        "        dst = os.path.join(validation_pneumonia_dir, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDyDg-_bqmw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d9eb12-1b39-407a-e586-57e15dc9e296"
      },
      "source": [
        "print('total training normal images:', len(os.listdir(train_normal_dir)))\n",
        "print('total training pneumonia images:', len(os.listdir(train_pneumonia_dir)))\n",
        "\n",
        "print('total validation normal images:', len(os.listdir(validation_normal_dir)))\n",
        "print('total validation pneumonia images:', len(os.listdir(validation_pneumonia_dir)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training normal images: 1072\n",
            "total training pneumonia images: 1072\n",
            "total validation normal images: 269\n",
            "total validation pneumonia images: 269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "facOl3UPqmw8"
      },
      "source": [
        "# Structure of directory\n",
        "\n",
        "BRANCH = '├─'\n",
        "LAST_BRANCH = '└─'\n",
        "TAB = '│  '\n",
        "EMPTY_TAB = '   '\n",
        "\n",
        "\n",
        "def get_dir_list(path, placeholder=''):\n",
        "    folder_list = [folder for folder in os.listdir(path) if os.path.isdir(os.path.join(path, folder))]\n",
        "    file_list = [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
        "    result = ''\n",
        "    for folder in folder_list[:-1]:\n",
        "        result += placeholder + BRANCH + folder + '\\n'\n",
        "        result += get_dir_list(os.path.join(path, folder), placeholder + TAB)\n",
        "    if folder_list:\n",
        "        result += placeholder + (BRANCH if file_list else LAST_BRANCH) + folder_list[-1] + '\\n'\n",
        "        result += get_dir_list(os.path.join(path, folder_list[-1]), placeholder + (TAB if file_list else EMPTY_TAB))\n",
        "    return result\n",
        "if __name__ == '__main__':\n",
        "    print(get_dir_list('./new_set'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiN8ungSqmw8"
      },
      "source": [
        "## Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMMxaoV5qmw8"
      },
      "source": [
        "from glob import glob\n",
        "import cv2\n",
        "print(\"No Pneumonia\")\n",
        "multipleImages = glob('./new_set/train/NORMAL/**')\n",
        "i_ = 0\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "for l in multipleImages[:10]:\n",
        "    im = cv2.imread(l)\n",
        "    im = cv2.resize(im, (128, 128)) \n",
        "    plt.subplot(5, 5, i_+1) \n",
        "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
        "    i_ += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwOMcen9qmw9"
      },
      "source": [
        "print(\"Pneumonia\")\n",
        "multipleImages = glob('./new_set/train/PNEUMONIA/**')\n",
        "i_ = 0\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "for l in multipleImages[:10]:\n",
        "    im = cv2.imread(l)\n",
        "    im = cv2.resize(im, (128, 128)) \n",
        "    plt.subplot(5, 5, i_+1) \n",
        "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
        "    i_ += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlDC2Kzuqmw-"
      },
      "source": [
        "## DataGenerator and Image augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUkQcYcpsgoM",
        "outputId": "05e0f8cb-6a68-4fe7-afd1-d6ddfd4b6f0c"
      },
      "source": [
        "!pip install nbimporter"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nbimporter\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/0e/495105a9efce3106130d676ae290c933acdae89bc9ea6e2b2c8cd556a591/nbimporter-0.3.3-py3-none-any.whl\n",
            "Installing collected packages: nbimporter\n",
            "Successfully installed nbimporter-0.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf7oafdhqHRV"
      },
      "source": [
        "import nbimporter   # install it for load ipyb inJupyter\r\n",
        "#from my_dataGenerator import DataGenerator  # invoke custom data generator class\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import cv2\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "import math"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIsbNOQ0b2qL",
        "outputId": "7d915798-2988-4669-e32b-3a1cd87bd72c"
      },
      "source": [
        "import math\r\n",
        "f = open('/content/drive/MyDrive/lung/encoded/NORMAL/_0_793163.txt')\r\n",
        "#np.fromfile(f)\r\n",
        "x = np.loadtxt(f)\r\n",
        "x = x.astype(int)\r\n",
        "x.reshape(224,224,1) \r\n",
        "print (x)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 39  39  48 ...  49  74 103]\n",
            " [ 41  38  44 ...  53  72 102]\n",
            " [ 39  39  43 ...  50  68 104]\n",
            " ...\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmYlXmdrsWx9"
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\r\n",
        "    'Generates data for Keras'\r\n",
        "    def __init__(self, list_IDs, labels, batch_size=32, dim=(224,224), n_channels=1,\r\n",
        "                 n_classes=2, shuffle=True,color_mode='grayscale', flag = False):\r\n",
        "        'Initialization'\r\n",
        "        self.dim = dim\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.labels = labels\r\n",
        "        self.list_IDs = list_IDs\r\n",
        "        self.n_channels = n_channels\r\n",
        "        self.n_classes = n_classes\r\n",
        "        self.shuffle = shuffle\r\n",
        "        self.color_mode=color_mode,\r\n",
        "        self.on_epoch_end(),\r\n",
        "        self.flag = flag\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        'Denotes the number of batches per epoch'\r\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        'Generate one batch of data'\r\n",
        "        # Generate indexes of the batch\r\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n",
        "\r\n",
        "        # Find list of IDs\r\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\r\n",
        "\r\n",
        "        # Generate data\r\n",
        "        X, y = self.__data_generation(list_IDs_temp)\r\n",
        "\r\n",
        "        return X, y\r\n",
        "\r\n",
        "    def on_epoch_end(self):\r\n",
        "        'Updates indexes after each epoch'\r\n",
        "        self.indexes = np.arange(len(self.list_IDs))\r\n",
        "        if self.shuffle == True:\r\n",
        "            np.random.shuffle(self.indexes)\r\n",
        "\r\n",
        "    def __data_generation(self, list_IDs_temp):\r\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\r\n",
        "        # Initialization\r\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\r\n",
        "        y = np.empty((self.batch_size), dtype=int)\r\n",
        "\r\n",
        "        # Generate data\r\n",
        "        for i, ID in enumerate(list_IDs_temp):\r\n",
        "            # Store sample\r\n",
        "            X[i,] = self._load_grayscale_image(ID)\r\n",
        "            \r\n",
        "            # Store class\r\n",
        "            y[i] = self.labels[ID]\r\n",
        "\r\n",
        "        return X,y \r\n",
        "\r\n",
        "    def _load_grayscale_image(self, image_path):\r\n",
        "        \"\"\"Load grayscale image\r\n",
        "\r\n",
        "        :param image_path: path to image to load\r\n",
        "        :return: loaded image\r\n",
        "        \"\"\"\r\n",
        "        if self.flag:\r\n",
        "          img = np.loadtxt(image_path)\r\n",
        "          img = img.astype(int)\r\n",
        "          img = img.reshape(224,224,1)\r\n",
        "\r\n",
        "        else:\r\n",
        "          img = cv2.imread(image_path)\r\n",
        "          img = cv2.resize(img, (224,224))\r\n",
        "          img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\r\n",
        "          img = img.astype('float')\r\n",
        "          img = img.reshape(224,224,1) \r\n",
        "          img = img / 255\r\n",
        "\r\n",
        "        return img"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73cE02kkqTMq"
      },
      "source": [
        "train_0=imageList_train_normal+imageList_val_normal   # all normal images path\r\n",
        "lable_train_0=[]\r\n",
        "    \r\n",
        "lable_train_0 = dict.fromkeys(train_0, 0)             # set normal as 0\r\n",
        "\r\n",
        "\r\n",
        "train_1=imageList_train_pneumonia+imageList_val_pneumonia  # all pneumonia  images path\r\n",
        "lable_train_1=[]\r\n",
        "   \r\n",
        "lable_train_1 = dict.fromkeys(train_1, 1)      # set pneumonia as 1\r\n",
        "\r\n",
        "lables = {**lable_train_0, **lable_train_1}    # Store both labels into a dictionary \r\n",
        "\r\n",
        "test_0=imageList_test_normal           # all test images path\r\n",
        "lable_test_0=[]\r\n",
        "\r\n",
        "lable_test_0 = dict.fromkeys(test_0, 0)             # set normal as 0\r\n",
        "\r\n",
        "test_1=imageList_test_pneumonia\r\n",
        "lable_test_1=[]\r\n",
        "\r\n",
        "lable_test_1 = dict.fromkeys(test_1, 1)      # set pneumonia as 1\r\n",
        "\r\n",
        "test_lables = {**lable_test_0, **lable_test_1}\r\n",
        "\r\n",
        "# lables format as {'./new_set/train/NORMAL\\\\IM-0115-0001.jpeg': 0, './new_set/train/NORMAL\\\\NORMAL2-IM-0533-0001.jpeg': 1, ...}\r\n",
        "\r\n",
        "\r\n",
        "from collections import defaultdict  # install it for easy operate dictionary array\r\n",
        "\r\n",
        "train_d=imageList_train_normal+imageList_train_pneumonia       # all train images path\r\n",
        "val_d=imageList_val_normal+imageList_val_pneumonia             # all val images path\r\n",
        "\r\n",
        "test_d=imageList_test_normal+imageList_test_pneumonia          # all test_d images path\r\n",
        "\r\n",
        "partition = defaultdict(list)         #init a dictionary\r\n",
        "for j in train_d:\r\n",
        "    partition['train'].append(j)      # Train as key and all train images path as value\r\n",
        "for j in val_d:\r\n",
        "    partition['validation'].append(j) # validation as key and all val images path as value\r\n",
        "for j in test_d:\r\n",
        "    partition['test'].append(j)       # test as key and all val images path as value\r\n",
        "    \r\n",
        "#  partition format as  {'train': ['./new_set/train/NORMAL\\\\IM-0115-0001.jpeg', ..], \r\n",
        "# 'validation': ['./new_set/train/NORMAL\\\\NORMAL2-IM-0533-0001.jpeg']}"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14l54pH5qmw-"
      },
      "source": [
        "# Parameters\n",
        "params = {'dim': (224,224),\n",
        "          'batch_size': 32,\n",
        "          'n_classes': 2,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Datasets\n",
        "partition = partition   # path as IDs\n",
        "labels = lables         # Labels\n",
        "\n",
        "# Call DataGenerator function from custom class \n",
        "training_generator = DataGenerator(partition['train'], labels, **params)\n",
        "validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
        "test_generator = DataGenerator(partition['test'], test_lables, **params, flag=True)"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtdBGSmdNljs",
        "outputId": "bed067d6-5955-4174-bae7-0a0c0202fa0c"
      },
      "source": [
        "image_width = 224\r\n",
        "image_height = 224\r\n",
        "batch=32\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "    rescale=1./255,\r\n",
        "    rotation_range=40,\r\n",
        "    width_shift_range=0.3,\r\n",
        "    height_shift_range=0.3,\r\n",
        "    shear_range=0.3,\r\n",
        "    zoom_range=0.3,\r\n",
        "    horizontal_flip=True,fill_mode = 'nearest',\r\n",
        "    \r\n",
        "    #channel_shift_range=13,data_format='channels_last',\r\n",
        ")\r\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\r\n",
        "\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "    train_dir,\r\n",
        "    target_size=(image_width, image_height),\r\n",
        "    batch_size=batch,\r\n",
        "    color_mode='grayscale',     # Channel 3 to channel 1\r\n",
        "    class_mode='binary',\r\n",
        ")\r\n",
        "\r\n",
        "val_generator = test_datagen.flow_from_directory(\r\n",
        "    validation_dir,\r\n",
        "    target_size=(image_width, image_height),\r\n",
        "    batch_size=batch,          # Channel 3 to channel 1\r\n",
        "     color_mode='grayscale',\r\n",
        "    class_mode='binary'\r\n",
        ")\r\n",
        "\r\n",
        "all_generator = train_datagen.flow_from_directory(\r\n",
        "    original_dataset_dir,\r\n",
        "    target_size=(image_width, image_height),\r\n",
        "    batch_size=batch,          # Channel 3 to channel 1\r\n",
        "     color_mode='grayscale',\r\n",
        "    class_mode='binary'\r\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2144 images belonging to 2 classes.\n",
            "Found 538 images belonging to 2 classes.\n",
            "Found 2682 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cabAP9zPKWdG"
      },
      "source": [
        "# Custom Layers\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g_gXj7-MEGS"
      },
      "source": [
        "from math import ceil\r\n",
        "from keras.utils import conv_utils\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import activations\r\n",
        "from keras import backend as K\r\n",
        "import keras.backend as K\r\n",
        "from keras import activations\r\n",
        "from tensorflow.python.distribute.sharded_variable import ShardedVariable\r\n",
        "from tensorflow.python.ops.variables import PartitionedVariable\r\n",
        "from typing import Any, Union, Tuple"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDLyhTPEKa7D"
      },
      "source": [
        "@tf.keras.utils.register_keras_serializable()\r\n",
        "class MaxPooling(Layer):\r\n",
        "    def __init__(self, pool_size=(2, 2), strides=None, padding='VALID', **kwargs):\r\n",
        "        super(MaxPooling, self).__init__(**kwargs)\r\n",
        "        self.pool_size = pool_size\r\n",
        "        if strides is not None:\r\n",
        "            self.strides = strides\r\n",
        "        else:\r\n",
        "            self.strides = pool_size\r\n",
        "        self.padding = padding\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        super(MaxPooling, self).build(input_shape)\r\n",
        "\r\n",
        "    def call(self, x, **kwargs):\r\n",
        "        y = tf.nn.max_pool2d(x, self.pool_size, self.strides, self.padding)\r\n",
        "        return y\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        rows = ceil(float(input_shape[1]) / float(self.strides[0]))\r\n",
        "        cols = ceil(float(input_shape[2]) / float(self.strides[1]))\r\n",
        "        return input_shape[0], rows, cols, input_shape[-1]\r\n",
        "\r\n",
        "    #def get_config(self):\r\n",
        "    #    config = {'pool_size': self.pool_size,\r\n",
        "    #              'padding': self.padding,\r\n",
        "    #              'strides': self.strideStride,}\r\n",
        "    #             # 'data_format': self.data_format}\r\n",
        "\r\n",
        "    #    base_config = super(MaxPooling, self).get_config()\r\n",
        "    #    return dict(list(base_config.items()) + list(config.items()))\r\n",
        "\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = super().get_config()\r\n",
        "        config.update({\r\n",
        "            'pool_size': self.pool_size,\r\n",
        "            \"strides\": self.strides,\r\n",
        "            \"padding\": self.padding,\r\n",
        "        })\r\n",
        "        return config\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzIeXZ2OLfSd"
      },
      "source": [
        "@tf.keras.utils.register_keras_serializable()\r\n",
        "class Dense(Layer):\r\n",
        "    def __init__(self, units=32, activation='relu', **kwargs):\r\n",
        "        super(Dense, self).__init__(**kwargs)\r\n",
        "        self.units = units\r\n",
        "        self.b = None  # b is initialized in build\r\n",
        "        self.w = None  # w is initialized in build\r\n",
        "        if activation is not None:\r\n",
        "            self.activation = activations.get(activation)\r\n",
        "        else:\r\n",
        "            self.activation = None\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.b = self.add_weight(\r\n",
        "            shape=(self.units,),\r\n",
        "            initializer=tf.keras.initializers.zeros(),\r\n",
        "            trainable=True,\r\n",
        "            dtype='float32',\r\n",
        "            name='dense_bias',\r\n",
        "        )\r\n",
        "        self.w = self.add_weight(\r\n",
        "            shape=(input_shape[-1], self.units),\r\n",
        "            trainable=True,\r\n",
        "            initializer=tf.keras.initializers.random_normal(),\r\n",
        "            dtype='float32',\r\n",
        "            name=\"dense_weights\",\r\n",
        "        )\r\n",
        "        super(Dense, self).build(input_shape)\r\n",
        "\r\n",
        "    def call(self, x, **kwargs):\r\n",
        "        y = K.dot(x, self.w) + self.b\r\n",
        "        if self.activation is not None:\r\n",
        "            y = self.activation(y)\r\n",
        "        return y\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        return input_shape[0], self.units\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = super().get_config()\r\n",
        "        config.update({\r\n",
        "            'units': self.units,\r\n",
        "            \"activation\": self.activation,\r\n",
        "        })\r\n",
        "        return config"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwu3NR_QL6Xj"
      },
      "source": [
        "@tf.keras.utils.register_keras_serializable()\r\n",
        "class Conv2D(Layer):\r\n",
        "    bias: Union[Union[PartitionedVariable, ShardedVariable, Conv2D], Any]\r\n",
        "    kernel: Union[Union[PartitionedVariable, ShardedVariable, Conv2D], Any]\r\n",
        "\r\n",
        "    def __init__(self, filters: int = 32, kernel_size: Tuple[int, int] = (3, 3), strides: Tuple[int, int] = (1, 1),\r\n",
        "                 padding: bool = 'VALID',\r\n",
        "                 activation='relu',\r\n",
        "                 dilation_rate=(1, 1), batch_size=1, **kwargs):\r\n",
        "\r\n",
        "        self.filters = filters\r\n",
        "        self.bias = None\r\n",
        "        self.kernel_size = kernel_size\r\n",
        "        self.kernel = None\r\n",
        "        self.strides = strides\r\n",
        "        self.padding = padding\r\n",
        "        if activation is not None:\r\n",
        "            self.activation = activations.get(activation)\r\n",
        "        else:\r\n",
        "            self.activation = None\r\n",
        "        self.dilation_rate = dilation_rate\r\n",
        "        self.batch_size = batch_size\r\n",
        "        if K.image_data_format() == 'channels_first':\r\n",
        "            self.channel_axis = 0\r\n",
        "        else:\r\n",
        "            self.channel_axis = -1\r\n",
        "        super(Conv2D, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        shape = (self.kernel_size[0], self.kernel_size[1], (input_shape[self.channel_axis]), self.filters)\r\n",
        "        self.bias = self.add_weight(name='conv_bias',\r\n",
        "                                    shape=(self.filters,),\r\n",
        "                                    dtype='float32',\r\n",
        "                                    initializer=tf.zeros_initializer(),\r\n",
        "                                    trainable=True)\r\n",
        "        self.kernel = self.add_weight(name='kernel',\r\n",
        "                                      shape=shape,\r\n",
        "                                      initializer=tf.keras.initializers.GlorotUniform(),\r\n",
        "                                      trainable=True)\r\n",
        "        super(Conv2D, self).build(input_shape)\r\n",
        "\r\n",
        "    def call(self, x, **kwargs):\r\n",
        "        y = tf.keras.backend.conv2d(x, self.kernel)\r\n",
        "        y = K.bias_add(y, self.bias)\r\n",
        "        if self.activation is not None:\r\n",
        "            y = self.activation(y)\r\n",
        "        return y\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        batch_size = input_shape[0]\r\n",
        "        convX = conv_utils.conv_output_length(\r\n",
        "            input_shape[1],\r\n",
        "            self.kernel_size[0],\r\n",
        "            padding=self.padding,\r\n",
        "            stride=self.strides[0],\r\n",
        "            dilation=self.dilation_rate[0]\r\n",
        "        )\r\n",
        "        convY = conv_utils.conv_output_length(\r\n",
        "            input_shape[2],\r\n",
        "            self.kernel_size[1],\r\n",
        "            padding=self.padding,\r\n",
        "            stride=self.strides[1],\r\n",
        "            dilation=self.dilation_rate[1]\r\n",
        "        )\r\n",
        "        return batch_size, convX, convY, self.filters\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = super().get_config()\r\n",
        "        config.update({\r\n",
        "            \"filters\": self.filters,\r\n",
        "            \"kernel_size\": self.kernel_size,\r\n",
        "            \"strides\": self.strides,\r\n",
        "            \"padding\": self.padding,\r\n",
        "            \"activation\": self.activation,\r\n",
        "            \"dilation_rate\": self.dilation_rate,\r\n",
        "            \"batch_size\": self.batch_size,\r\n",
        "        })\r\n",
        "        return config\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpUc9LJO5l4I"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MEv26XxqmxB"
      },
      "source": [
        "import keras\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow import keras\r\n",
        "from keras import layers\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.layers import Input, Dropout, SpatialDropout2D, experimental\r\n",
        "\r\n",
        "model = keras.Sequential([\r\n",
        "\r\n",
        "    keras.layers.Input(shape=(image_width, image_height, 1)),\r\n",
        "    Conv2D(filters = 64, padding=\"zero\", strides = (3,3)),\r\n",
        "    SpatialDropout2D(0.2),\r\n",
        "    BatchNormalization(),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "\r\n",
        "\r\n",
        "    Conv2D(filters = 128, padding=\"zero\", strides = (3,3)),\r\n",
        "    SpatialDropout2D(0.2),\r\n",
        "    BatchNormalization(),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "\r\n",
        "\r\n",
        "    Conv2D(filters = 256, padding=\"zero\", strides = (3,3)),\r\n",
        "    SpatialDropout2D(0.2),\r\n",
        "    BatchNormalization(),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "\r\n",
        "\r\n",
        "    Conv2D(filters = 512, padding=\"zero\", strides = (3,3)),\r\n",
        "    SpatialDropout2D(0.2),\r\n",
        "    BatchNormalization(),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "\r\n",
        "\r\n",
        "    Conv2D(filters = 1024, padding=\"zero\", strides = (3,3)),\r\n",
        "    SpatialDropout2D(0.2),\r\n",
        "    BatchNormalization(),\r\n",
        "    MaxPooling(pool_size=(2,2), strides = (2,2)),\r\n",
        "\r\n",
        "\r\n",
        "    layers.Flatten(),\r\n",
        "    Dense(units=2048),\r\n",
        "    BatchNormalization(),\r\n",
        "\r\n",
        "    Dense(units=1, activation = \"sigmoid\"),\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "model.compile(optimizer='rmsprop',\r\n",
        "              loss='binary_crossentropy',\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWScRVvt70a5",
        "outputId": "41ae029d-d73f-44b8-8f3d-51c5ad8395ac"
      },
      "source": [
        "patience = 5\r\n",
        "\r\n",
        "cb = keras.callbacks.EarlyStopping(\r\n",
        "    monitor='val_loss', min_delta=0, patience=patience, verbose=0, mode='auto',\r\n",
        "    baseline=None, restore_best_weights=False\r\n",
        ")\r\n",
        "model.save('test.h5')\r\n",
        "model.save_weights('zero.h5')\r\n",
        "history = model.fit(train_generator,\r\n",
        "                  epochs=1000, #we stop training the model with early stopping\r\n",
        "                  validation_data=val_generator,\r\n",
        "                  callbacks=[cb]\r\n",
        "                  )\r\n",
        "\r\n",
        "model.save(save.h5)\r\n",
        "model.save_weights('model_first.h5')\r\n",
        "\r\n",
        "#retrain on the whole dataset for the same epochs as before\r\n",
        "model.load_weights('def.h5')\r\n",
        "epochs = (len (history.history['loss']) - patience)\r\n",
        "\r\n",
        "history = model.fit(all_generator,\r\n",
        "                  epochs = epochs)\r\n",
        "\r\n",
        "\r\n",
        "model.save_weights('model_w.h5')\r\n",
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "67/67 [==============================] - 542s 8s/step - loss: 4.1220 - accuracy: 0.5856 - val_loss: 8.9615 - val_accuracy: 0.5000\n",
            "Epoch 2/1000\n",
            "67/67 [==============================] - 43s 647ms/step - loss: 0.8376 - accuracy: 0.6897 - val_loss: 1.1200 - val_accuracy: 0.5000\n",
            "Epoch 3/1000\n",
            "67/67 [==============================] - 43s 646ms/step - loss: 0.7549 - accuracy: 0.7176 - val_loss: 0.9076 - val_accuracy: 0.5000\n",
            "Epoch 4/1000\n",
            "67/67 [==============================] - 43s 647ms/step - loss: 0.5554 - accuracy: 0.7576 - val_loss: 0.6529 - val_accuracy: 0.8253\n",
            "Epoch 5/1000\n",
            "67/67 [==============================] - 43s 647ms/step - loss: 0.5442 - accuracy: 0.7838 - val_loss: 1.0130 - val_accuracy: 0.5019\n",
            "Epoch 6/1000\n",
            "67/67 [==============================] - 43s 646ms/step - loss: 0.5994 - accuracy: 0.7578 - val_loss: 0.9721 - val_accuracy: 0.5000\n",
            "Epoch 7/1000\n",
            "67/67 [==============================] - 43s 645ms/step - loss: 0.4925 - accuracy: 0.8056 - val_loss: 1.9474 - val_accuracy: 0.5000\n",
            "Epoch 8/1000\n",
            "67/67 [==============================] - 43s 644ms/step - loss: 0.4380 - accuracy: 0.8127 - val_loss: 0.7101 - val_accuracy: 0.5743\n",
            "Epoch 9/1000\n",
            "67/67 [==============================] - 43s 647ms/step - loss: 0.5259 - accuracy: 0.7942 - val_loss: 0.2937 - val_accuracy: 0.8903\n",
            "Epoch 10/1000\n",
            "67/67 [==============================] - 43s 644ms/step - loss: 0.4259 - accuracy: 0.8273 - val_loss: 0.5573 - val_accuracy: 0.6970\n",
            "Epoch 11/1000\n",
            "67/67 [==============================] - 44s 650ms/step - loss: 0.3983 - accuracy: 0.8395 - val_loss: 0.2884 - val_accuracy: 0.8755\n",
            "Epoch 12/1000\n",
            "34/67 [==============>...............] - ETA: 17s - loss: 0.4068 - accuracy: 0.8332"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DODU1lmkTjjk"
      },
      "source": [
        "model = keras.models.load_model('test.h5')"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkYYuYgQqmw_"
      },
      "source": [
        "# Task 3 - Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-YVkZvbqmw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44528fb-fb83-4381-ab45-c672111e5258"
      },
      "source": [
        "# Display how many test data respect to normal and pneumonia\n",
        "test_dir = 'drive/MyDrive/lung/encoded'\n",
        "\n",
        "test_normal_dir = os.path.join(test_dir, 'NORMAL')\n",
        "test_pneumonia_dir = os.path.join(test_dir, 'PNEUMONIA')\n",
        "\n",
        "print('total test normal images:', len(os.listdir(test_normal_dir)))\n",
        "print('total test pneumonia images:', len(os.listdir(test_pneumonia_dir)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total test normal images: 30\n",
            "total test pneumonia images: 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjSj343zqmxA"
      },
      "source": [
        "# Read the test text file and store into list\n",
        "# And split into x_test and y_test for labels\n",
        "test_dir = 'drive/MyDrive/lung/encoded'\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['NORMAL', 'PNEUMONIA']:    \n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            tex = []\n",
        "            for line in f:\n",
        "                row = line.split()\n",
        "                for r in row:\n",
        "                    r1 = float(r)  # Convert string to float\n",
        "                    tex.append(r1) # (50176)\n",
        "                                               \n",
        "            texts.append(tex)      # (51*50176)\n",
        "            if label_type == 'NORMAL':\n",
        "                labels.append(0)   # Labled Normal to 0     \n",
        "            else:\n",
        "                labels.append(1)   # Labled PNEUMONIA to 1         "
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeqYQakpqmxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc90b5c-8898-46d1-d7d2-42b8aa69245f"
      },
      "source": [
        "# Convert list to np array in order to reshape\n",
        "x_test=np.array(texts)\n",
        "y_test=np.array(labels)\n",
        "print(x_test.shape)\n",
        "\n",
        "# Reshape as the same format as train\n",
        "x_test=x_test.reshape(51,224,224,1) \n",
        "print(x_test.shape)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(51, 50176)\n",
            "(51, 224, 224, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGqDhhq3_B-6"
      },
      "source": [
        "# Parameters\r\n",
        "params = {'dim': (224,224),\r\n",
        "          'batch_size': 32,\r\n",
        "          'n_classes': 2,\r\n",
        "          'n_channels': 1,\r\n",
        "          'shuffle': True}\r\n",
        "\r\n",
        "# Datasets\r\n",
        "partition = partition   # path as IDs\r\n",
        "labels = lables         # Labels\r\n",
        "\r\n",
        "# Call DataGenerator function from custom class \r\n",
        "eval_generator = DataGenerator(x_test, y_test, **params)\r\n",
        "#all_generator = DataGenerator(partition['all'], labels, **params)"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "y7qOZaUeqmxB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "outputId": "9f758bab-9a44-4417-b438-e44fbfa79572"
      },
      "source": [
        "#model_2.load_weights('model.h5')\n",
        "# Choose one of them\n",
        "##test_loss, test_acc =model_2.evaluate(x_test, y_test, steps=1)\n",
        "\n",
        "#print('test acc:', test_acc)\n",
        "model.evaluate(test_generator)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py:583: RuntimeWarning: divide by zero encountered in log10\n",
            "  numdigits = int(np.log10(self.target)) + 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OverflowError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-211-0eef905438d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print('test acc:', test_acc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1394\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1396\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_called_in_fit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_predict_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_finalize_progbar\u001b[0;34m(self, logs, counter)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mnumdigits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'%'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumdigits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'd/%d ['\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0mprog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
          ]
        }
      ]
    }
  ]
}